{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a report file which can be used in a variety of manners, including for use in visual profiling with Nsight Systems, which we will look at in more detail in the following section.\n",
    "\n",
    "Here we use the `--stats=true` flag to indicate we would like summary statistics printed. In this section this summary will be the focus of our attention. There is quite a lot of information printed:\n",
    "\n",
    "- Operating System Runtime Summary (`osrt_sum`)\n",
    "- **CUDA API Summary (`cuda_api_sum`)**\n",
    "- **CUDA Kernel Summary (`cuda_gpu_kern_sum`)**\n",
    "- **CUDA Memory Time Operation Summary (`cuda_gpu_mem_time_sum`)**\n",
    "- **CUDA Memory Size Operation Summary (`cuda_gpu_mem_size_sum`)**\n",
    "\n",
    "In this section you will primarily be using the 4 summaries in **bold** above. In the next section, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `cuda_gpu_kern_sum` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-ff93.qdstrm'\n",
      "[1/8] [========================100%] report1.nsys-rep\n",
      "[2/8] [========================100%] report1.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6155648078        318  19357383.9  10076477.5     31300  100154857   27664322.4  poll                  \n",
      "      8.8        599863816        283   2119660.1   2066809.0       130   20523847    1592099.6  sem_timedwait         \n",
      "      0.5         31142840        499     62410.5     10990.0       390    8594107     403907.0  ioctl                 \n",
      "      0.3         19555164         24    814798.5      5120.0       830    7775983    2211245.3  mmap                  \n",
      "      0.0           878173         27     32524.9      3780.0      2910     541172     102474.7  mmap64                \n",
      "      0.0           499423         44     11350.5     10235.5      3490      29360       4850.9  open64                \n",
      "      0.0           192400          4     48100.0     46205.0     33990      66000      15995.7  pthread_create        \n",
      "      0.0           167520         29      5776.6      3600.0      1700      29520       5875.7  fopen                 \n",
      "      0.0           141492         11     12862.9     14371.0      3880      20750       5695.3  write                 \n",
      "      0.0            63850         12      5320.8      3360.0      1460      17090       5171.3  munmap                \n",
      "      0.0            50841         26      1955.4        70.0        60      49071       9609.7  fgets                 \n",
      "      0.0            37980          6      6330.0      6815.0      2420       8780       2324.1  open                  \n",
      "      0.0            34950         52       672.1       505.0       150       4550        651.1  fcntl                 \n",
      "      0.0            25840         22      1174.5      1010.0       510       3330        634.9  fclose                \n",
      "      0.0            23660         14      1690.0      1355.0       780       4010        885.2  read                  \n",
      "      0.0            16190          2      8095.0      8095.0      4900      11290       4518.4  socket                \n",
      "      0.0            12510          5      2502.0      1790.0        90       6230       2657.3  fread                 \n",
      "      0.0            10180          1     10180.0     10180.0     10180      10180          0.0  connect               \n",
      "      0.0             6170         64        96.4       120.0        40        360         58.5  pthread_mutex_trylock \n",
      "      0.0             4860          1      4860.0      4860.0      4860       4860          0.0  pipe2                 \n",
      "      0.0             2770          1      2770.0      2770.0      2770       2770          0.0  bind                  \n",
      "      0.0             1410          1      1410.0      1410.0      1410       1410          0.0  listen                \n",
      "      0.0              220          1       220.0       220.0       220        220          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     95.0       2467551953          1  2467551953.0  2467551953.0  2467551953  2467551953          0.0  cudaDeviceSynchronize\n",
      "      4.2        110323800          3    36774600.0       29450.0       14940   110279410   63657033.2  cudaMallocManaged    \n",
      "      0.8         19606984          3     6535661.3     5984156.0     5815174     7807654    1104813.4  cudaFree             \n",
      "      0.0           125361          1      125361.0      125361.0      125361      125361          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2467611875          1  2467611875.0  2467611875.0  2467611875  2467611875          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     76.2         35356389   2304   15345.7    6111.5      1823    202397      23913.0  [CUDA Unified Memory memcpy HtoD]\n",
      "     23.8         11066656    768   14409.7    4063.0      1182     84927      22895.9  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report1.nsys-rep\n",
      "    /dli/task/report1.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-94ae.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.1       4527843945        237  19104826.8  10071309.0      2991  100152420   27416832.5  poll                  \n",
      "      8.8        442619358        211   2097722.1   2066327.0       180   20431238    1503884.3  sem_timedwait         \n",
      "      0.7         35136273        498     70554.8     10880.5       390    7968711     463638.8  ioctl                 \n",
      "      0.4         19097603         24    795733.5      4951.0       980    7241361    2146200.0  mmap                  \n",
      "      0.0           867156         27     32116.9      4000.0      3131     540705     102449.5  mmap64                \n",
      "      0.0           488815         44     11109.4     10805.5      3570      30270       4794.9  open64                \n",
      "      0.0           200765          4     50191.3     52336.0     30341      65752      14675.4  pthread_create        \n",
      "      0.0           155433         29      5359.8      3900.0      1650      26790       5289.6  fopen                 \n",
      "      0.0           152163         11     13833.0     13781.0      1150      20310       4914.3  write                 \n",
      "      0.0           116925         11     10629.5      3630.0      1420      44342      14549.3  munmap                \n",
      "      0.0            50052         26      1925.1        70.0        60      48292       9457.0  fgets                 \n",
      "      0.0            36591          6      6098.5      7395.5      2440       8100       2361.9  open                  \n",
      "      0.0            34171         52       657.1       495.0       200       4810        676.9  fcntl                 \n",
      "      0.0            24900         22      1131.8       975.0       470       2800        595.2  fclose                \n",
      "      0.0            20960         14      1497.1      1305.0       730       3440        833.1  read                  \n",
      "      0.0            17131          2      8565.5      8565.5      4000      13131       6456.6  socket                \n",
      "      0.0            11860          1     11860.0     11860.0     11860      11860          0.0  connect               \n",
      "      0.0             7070          5      1414.0      1060.0        90       3340       1369.3  fread                 \n",
      "      0.0             6290          1      6290.0      6290.0      6290       6290          0.0  pipe2                 \n",
      "      0.0             5611         64        87.7        50.0        40        330         55.8  pthread_mutex_trylock \n",
      "      0.0             2210          1      2210.0      2210.0      2210       2210          0.0  bind                  \n",
      "      0.0             1360          1      1360.0      1360.0      1360       1360          0.0  listen                \n",
      "      0.0              260          1       260.0       260.0       260        260          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     92.2       1589235342          1  1589235342.0  1589235342.0  1589235342  1589235342          0.0  cudaDeviceSynchronize\n",
      "      6.7        115972760          3    38657586.7       29621.0       15050   115928089   66918218.4  cudaMallocManaged    \n",
      "      1.1         19204103          3     6401367.7     6069479.0     5850762     7283862     772046.9  cudaFree             \n",
      "      0.0           108413          1      108413.0      108413.0      108413      108413          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       1589285677          1  1589285677.0  1589285677.0  1589285677  1589285677          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.6         34678382   2304   15051.4    5838.5      1823     81536      22518.3  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.4         11217931    768   14606.7    4047.5      1183    140802      23572.6  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "`\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-8e76.qdstrm'\n",
      "[1/8] [========================100%] report5.nsys-rep\n",
      "[2/8] [========================100%] report5.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report5.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     88.1       1798046505        100  17980465.1  10071933.0      2300  100722867   26430777.9  poll                  \n",
      "      9.3        190153516         89   2136556.4   2067649.0       190   20480920    2266865.3  sem_timedwait         \n",
      "      1.6         31659860        497     63701.9     10970.0       400    8451140     402482.5  ioctl                 \n",
      "      0.9         19066869         24    794452.9      5800.0      1110    7154561    2138676.0  mmap                  \n",
      "      0.0           934529         27     34612.2      4300.0      3210     556007     105285.0  mmap64                \n",
      "      0.0           510485         44     11601.9     10515.5      3770      33240       6212.7  open64                \n",
      "      0.0           182652         29      6298.3      3610.0      1510      49501       9143.0  fopen                 \n",
      "      0.0           171544          4     42886.0     43706.0     32501      51631      10124.3  pthread_create        \n",
      "      0.0           152181         11     13834.6     14420.0      1080      20490       4828.2  write                 \n",
      "      0.0            76721         12      6393.4      2990.0      1110      45731      12436.0  munmap                \n",
      "      0.0            55600         26      2138.5        70.0        60      53840      10545.1  fgets                 \n",
      "      0.0            43991          6      7331.8      7675.0      2910      12270       3487.2  open                  \n",
      "      0.0            33780         52       649.6       495.0       160       5070        696.7  fcntl                 \n",
      "      0.0            26490         22      1204.1       950.0       540       3520        716.9  fclose                \n",
      "      0.0            23050         14      1646.4      1235.0       990       5150       1180.6  read                  \n",
      "      0.0            17830          2      8915.0      8915.0      4430      13400       6342.7  socket                \n",
      "      0.0            12990          1     12990.0     12990.0     12990      12990          0.0  connect               \n",
      "      0.0             7670          1      7670.0      7670.0      7670       7670          0.0  pipe2                 \n",
      "      0.0             6570          5      1314.0      1180.0        60       2970       1223.6  fread                 \n",
      "      0.0             5770         64        90.2        50.0        40        380         59.5  pthread_mutex_trylock \n",
      "      0.0             2450          1      2450.0      2450.0      2450       2450          0.0  bind                  \n",
      "      0.0             1220          1      1220.0      1220.0      1220       1220          0.0  listen                \n",
      "      0.0              370          1       370.0       370.0       370        370          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     48.7        111087534          3  37029178.0     28101.0     14550  111044883   64099481.2  cudaMallocManaged    \n",
      "     42.9         97797035          1  97797035.0  97797035.0  97797035   97797035          0.0  cudaDeviceSynchronize\n",
      "      8.4         19051970          3   6350656.7   6071626.0   5798402    7181942     732761.4  cudaFree             \n",
      "      0.0            52791          1     52791.0     52791.0     52791      52791          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         97788757          1  97788757.0  97788757.0  97788757  97788757          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.6         34462056   2405   14329.3   11265.0      1791    160832      18899.1  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.4         11148627    768   14516.4    4255.5      1343    174239      23398.4  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2405     0.167     0.131     0.004     0.946        0.242  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report5.nsys-rep\n",
      "    /dli/task/report5.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\n",
      "Number of SMs: 80\n",
      "Compute Capability Major: 8\n",
      "Compute Capability Minor: 6\n",
      "Warp Size: 32\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-e366.qdstrm'\n",
      "[1/8] [========================100%] report6.nsys-rep\n",
      "[2/8] [========================100%] report6.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report6.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.5       1780896475         99  17988853.3  10074423.0      2040  100148485   26494063.9  poll                  \n",
      "      9.6        195100349         89   2192138.8   2067376.0       210   20466321    2578973.7  sem_timedwait         \n",
      "      1.9         38213257        497     76887.8     12470.0       370    9522303     524909.0  ioctl                 \n",
      "      0.9         19002926         24    791788.6      4805.0      1070    7157173    2135846.5  mmap                  \n",
      "      0.1          1107969         27     41035.9      4170.0      3650     753563     143059.0  mmap64                \n",
      "      0.0           521743         44     11857.8     10745.0      4510      32301       5069.8  open64                \n",
      "      0.0           195214          4     48803.5     48111.0     37381      61611      13173.3  pthread_create        \n",
      "      0.0           189342         29      6529.0      3970.0      1550      46881       8732.4  fopen                 \n",
      "      0.0           152895         11     13899.5     16220.0       800      19020       5187.1  write                 \n",
      "      0.0            79112         11      7192.0      3130.0      1180      46251      13021.9  munmap                \n",
      "      0.0            58121         26      2235.4        90.0        80      55821      10929.4  fgets                 \n",
      "      0.0            40481          6      6746.8      6070.5      3350       9660       2453.4  open                  \n",
      "      0.0            36250         52       697.1       530.0       170       5980        815.6  fcntl                 \n",
      "      0.0            32020         22      1455.5      1250.0       750       4300        810.0  fclose                \n",
      "      0.0            20041         14      1431.5      1285.0       450       4020       1023.3  read                  \n",
      "      0.0            15500          2      7750.0      7750.0      3700      11800       5727.6  socket                \n",
      "      0.0            11460          1     11460.0     11460.0     11460      11460          0.0  connect               \n",
      "      0.0             8210          5      1642.0      1590.0        80       3170       1421.6  fread                 \n",
      "      0.0             6600         64       103.1        90.0        40        440         83.4  pthread_mutex_trylock \n",
      "      0.0             5940          1      5940.0      5940.0      5940       5940          0.0  pipe2                 \n",
      "      0.0             2080          1      2080.0      2080.0      2080       2080          0.0  bind                  \n",
      "      0.0             1220          1      1220.0      1220.0      1220       1220          0.0  listen                \n",
      "      0.0              240          1       240.0       240.0       240        240          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     50.8        116020427          3  38673475.7     29180.0     13870  115977377   66947142.8  cudaMallocManaged    \n",
      "     40.8         93297728          1  93297728.0  93297728.0  93297728   93297728          0.0  cudaDeviceSynchronize\n",
      "      8.3         19063597          3   6354532.3   6063244.0   5817750    7182603     727559.3  cudaFree             \n",
      "      0.0            46790          1     46790.0     46790.0     46790      46790          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         93286894          1  93286894.0  93286894.0  93286894  93286894          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     81.8         50632378  12550    4034.5    2079.0      1791    165248       8338.2  [CUDA Unified Memory memcpy HtoD]\n",
      "     18.2         11233282    768   14626.7    4143.5      1343    158080      23420.6  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  12550     0.032     0.004     0.004     1.036        0.106  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report6.nsys-rep\n",
      "    /dli/task/report6.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating '/tmp/nsys-report-22fb.qdstrm'\n",
      "[1/8] [========================100%] report13.nsys-rep\n",
      "[2/8] [========================100%] report13.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report13.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     82.1        542307132         38  14271240.3  10072346.0      2050  100133885   22064454.4  poll                  \n",
      "     11.4         75534746         34   2221610.2   2068418.0       180   20503774    3745500.8  sem_timedwait         \n",
      "      4.6         30620930        483     63397.4     11060.0       390    8264311     395700.3  ioctl                 \n",
      "      1.5         10119706         18    562205.9      7195.5       960    9988383    2352475.1  mmap                  \n",
      "      0.1           904478         27     33499.2      4290.0      3130     563127     106618.0  mmap64                \n",
      "      0.1           494091         44     11229.3     10475.0      3400      30491       4750.5  open64                \n",
      "      0.0           189846          4     47461.5     46651.5     35121      61422      13375.3  pthread_create        \n",
      "      0.0           171255         29      5905.3      3740.0      1570      35441       6970.6  fopen                 \n",
      "      0.0           147794         11     13435.8     13951.0       880      18080       4581.3  write                 \n",
      "      0.0            71842          7     10263.1      3890.0      2220      43012      14848.6  munmap                \n",
      "      0.0            50811         26      1954.3        70.0        60      49001       9595.7  fgets                 \n",
      "      0.0            37581          6      6263.5      6895.0      2530       7761       2021.6  open                  \n",
      "      0.0            35562         52       683.9       480.0       150       6510        876.7  fcntl                 \n",
      "      0.0            25931         22      1178.7      1080.0       490       3550        676.9  fclose                \n",
      "      0.0            21900         14      1564.3      1320.0       870       3870        882.0  read                  \n",
      "      0.0            17570          2      8785.0      8785.0      4920      12650       5465.9  socket                \n",
      "      0.0            12621          1     12621.0     12621.0     12621      12621          0.0  connect               \n",
      "      0.0             7730          5      1546.0      1250.0        90       3250       1426.4  fread                 \n",
      "      0.0             7320          1      7320.0      7320.0      7320       7320          0.0  pipe2                 \n",
      "      0.0             6860         64       107.2       120.0        40        420         71.1  pthread_mutex_trylock \n",
      "      0.0             2340          1      2340.0      2340.0      2340       2340          0.0  bind                  \n",
      "      0.0             1630          1      1630.0      1630.0      1630       1630          0.0  listen                \n",
      "      0.0              230          1       230.0       230.0       230        230          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     82.2        108593112          1  108593112.0  108593112.0  108593112  108593112          0.0  cudaMallocManaged    \n",
      "     10.2         13441119          1   13441119.0   13441119.0   13441119   13441119          0.0  cudaDeviceSynchronize\n",
      "      7.6         10098737          1   10098737.0   10098737.0   10098737   10098737          0.0  cudaFree             \n",
      "      0.0            25891          1      25891.0      25891.0      25891      25891          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)            Name          \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------\n",
      "    100.0         13436074          1  13436074.0  13436074.0  13436074  13436074          0.0  deviceKernel(int *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11270421    768   14675.0    4111.5      1375    155586      23582.4  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report13.nsys-rep\n",
      "    /dli/task/report13.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-500f.qdstrm'\n",
      "[1/8] [========================100%] report14.nsys-rep\n",
      "[2/8] [========================100%] report14.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report14.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.2       1785176803         99  18032088.9  10072400.0      2301  100141220   26410914.7  poll                  \n",
      "      9.7        198147319         89   2226374.4   2067112.0       130   20407879    2786265.1  sem_timedwait         \n",
      "      2.1         43196865        497     86915.2     14870.0       390    9518324     525497.5  ioctl                 \n",
      "      0.9         19140772         24    797532.2      4265.5      1150    7233796    2149970.4  mmap                  \n",
      "      0.1          1139673         27     42210.1      4310.0      3770     748352     141986.1  mmap64                \n",
      "      0.0           554686         44     12606.5     12331.0      4410      35911       5736.7  open64                \n",
      "      0.0           200167         29      6902.3      4511.0      1771      46512       8713.0  fopen                 \n",
      "      0.0           188766          4     47191.5     45351.5     40371      57692       8197.1  pthread_create        \n",
      "      0.0           169357         11     15396.1     14751.0       780      26561       7027.7  write                 \n",
      "      0.0            61200         12      5100.0      3500.0      1130      22180       5630.8  munmap                \n",
      "      0.0            59982         26      2307.0        90.0        70      57722      11302.5  fgets                 \n",
      "      0.0            45060          6      7510.0      7540.0      4520      10310       2304.5  open                  \n",
      "      0.0            35923         52       690.8       500.0       150       5940        806.6  fcntl                 \n",
      "      0.0            34161         22      1552.8      1285.0       770       4100        807.7  fclose                \n",
      "      0.0            20452         14      1460.9       950.0       360       4181       1152.0  read                  \n",
      "      0.0            17351          2      8675.5      8675.5      4520      12831       5876.8  socket                \n",
      "      0.0            12730          1     12730.0     12730.0     12730      12730          0.0  connect               \n",
      "      0.0             9950          5      1990.0      1590.0       150       4390       1879.1  fread                 \n",
      "      0.0             6670          1      6670.0      6670.0      6670       6670          0.0  pipe2                 \n",
      "      0.0             5740         64        89.7        50.0        40        510         69.6  pthread_mutex_trylock \n",
      "      0.0             2240          1      2240.0      2240.0      2240       2240          0.0  bind                  \n",
      "      0.0             1330          1      1330.0      1330.0      1330       1330          0.0  listen                \n",
      "      0.0              240          1       240.0       240.0       240        240          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     52.7        122753004          3  40917668.0     24321.0     13990  122714693   70838301.8  cudaMallocManaged    \n",
      "     39.1         90979787          1  90979787.0  90979787.0  90979787   90979787          0.0  cudaDeviceSynchronize\n",
      "      8.2         19165423          3   6388474.3   6089132.0   5790863    7285428     790971.2  cudaFree             \n",
      "      0.0            45981          1     45981.0     45981.0     45981      45981          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         90970250          1  90970250.0  90970250.0  90970250  90970250          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     81.8         50087161  12148    4123.1    2143.0      1790    178082       8569.1  [CUDA Unified Memory memcpy HtoD]\n",
      "     18.2         11177228    768   14553.7    4175.5      1310     92417      23059.0  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  12148     0.033     0.008     0.004     0.983        0.111  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report14.nsys-rep\n",
      "    /dli/task/report14.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-188b.qdstrm'\n",
      "[1/8] [========================100%] report15.nsys-rep\n",
      "[2/8] [========================100%] report15.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report15.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     80.2        491999237         33  14909067.8  10073413.0      2440  100136963   23442444.8  poll                  \n",
      "     11.0         67230393         29   2318289.4   2066590.0       150   20472174    4233744.6  sem_timedwait         \n",
      "      5.7         34832358        497     70085.2     11040.0       380    8157767     464956.7  ioctl                 \n",
      "      2.7         16753851         24    698077.1      4590.0      1160    8070614    1977498.5  mmap                  \n",
      "      0.2          1106693         27     40988.6      4220.0      3000     763748     145041.2  mmap64                \n",
      "      0.1           483462         44     10987.8     10175.5      3360      32280       4429.7  open64                \n",
      "      0.0           175844          4     43961.0     40511.0     30101      64721      15096.3  pthread_create        \n",
      "      0.0           174326         29      6011.2      3900.0      1620      38351       7243.4  fopen                 \n",
      "      0.0           151364         11     13760.4     14901.0       860      20030       4854.4  write                 \n",
      "      0.0            68692         11      6244.7      3800.0      1190      18410       5956.6  munmap                \n",
      "      0.0            49481         26      1903.1        70.0        60      47731       9347.1  fgets                 \n",
      "      0.0            39681          6      6613.5      7200.0      2500       9621       2847.2  open                  \n",
      "      0.0            33360         52       641.5       465.0       210       5140        694.6  fcntl                 \n",
      "      0.0            29160         22      1325.5      1085.0       550       3650        740.0  fclose                \n",
      "      0.0            21851         14      1560.8      1220.0       850       4310       1053.1  read                  \n",
      "      0.0            13921          5      2784.2      1150.0       150       6981       2912.2  fread                 \n",
      "      0.0            11320          2      5660.0      5660.0      4330       6990       1880.9  socket                \n",
      "      0.0             7510          1      7510.0      7510.0      7510       7510          0.0  pipe2                 \n",
      "      0.0             6440         64       100.6        60.0        40        420         67.1  pthread_mutex_trylock \n",
      "      0.0             5810          1      5810.0      5810.0      5810       5810          0.0  connect               \n",
      "      0.0             1760          1      1760.0      1760.0      1760       1760          0.0  bind                  \n",
      "      0.0             1030          1      1030.0      1030.0      1030       1030          0.0  listen                \n",
      "      0.0              270          1       270.0       270.0       270        270          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     69.8        112863760          3  37621253.3     30031.0     14890  112818839   65123019.9  cudaMallocManaged    \n",
      "     19.8         31997711          2  15998855.5  15998855.5    858511   31139200   21411680.5  cudaDeviceSynchronize\n",
      "     10.4         16791985          3   5597328.3   4349605.0   4320164    8122216    2186666.4  cudaFree             \n",
      "      0.0            51082          4     12770.5     11190.5      4410      24291       9369.8  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     97.3         31141147          3  10380382.3  10352095.0  10213053  10575999     183119.0  initWith(float, float *, int)                 \n",
      "      2.7           855877          1    855877.0    855877.0    855877    855877          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11271518    768   14676.5    4287.5      1343    174337      23924.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report15.nsys-rep\n",
      "    /dli/task/report15.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-ce7e.qdstrm'\n",
      "[1/8] [========================100%] report16.nsys-rep\n",
      "[2/8] [========================100%] report16.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report16.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     72.6        371922331         30  12397411.0  10068845.5      2151  100135966   18705018.5  poll                  \n",
      "     13.3         68165246         27   2524638.7   2064163.0       200   20803915    4962334.7  sem_timedwait         \n",
      "      7.1         36592294        500     73184.6     10485.0       380    8080929     459696.8  ioctl                 \n",
      "      3.5         17811163         29    614178.0      3210.0       920   17686650    3283505.0  fopen                 \n",
      "      3.1         15709627         24    654567.8      5480.0       850    7311053    1834070.5  mmap                  \n",
      "      0.2           896457         27     33202.1      3830.0      2900     543651     103219.2  mmap64                \n",
      "      0.1           473023         44     10750.5     10225.5      3210      32261       4705.8  open64                \n",
      "      0.0           151762          4     37940.5     38940.5     25200      48681       9822.6  pthread_create        \n",
      "      0.0           151323         11     13756.6     14390.0      1020      20680       5085.2  write                 \n",
      "      0.0            53841         11      4894.6      3540.0      1460      20571       5362.0  munmap                \n",
      "      0.0            51361         26      1975.4        70.0        50      49621       9717.8  fgets                 \n",
      "      0.0            37863          6      6310.5      6470.5      2420       9590       2556.2  open                  \n",
      "      0.0            32951         52       633.7       430.0       150       5610        770.9  fcntl                 \n",
      "      0.0            29860         22      1357.3      1025.0       520       4280        898.7  fclose                \n",
      "      0.0            22430         14      1602.1      1320.0       870       4160        964.2  read                  \n",
      "      0.0            14340          2      7170.0      7170.0      5020       9320       3040.6  socket                \n",
      "      0.0             9220          1      9220.0      9220.0      9220       9220          0.0  connect               \n",
      "      0.0             8541          5      1708.2      1691.0        80       2830       1035.8  fread                 \n",
      "      0.0             7350          1      7350.0      7350.0      7350       7350          0.0  pipe2                 \n",
      "      0.0             6410         64       100.2       120.0        40        360         61.0  pthread_mutex_trylock \n",
      "      0.0             2900          1      2900.0      2900.0      2900       2900          0.0  bind                  \n",
      "      0.0             1320          1      1320.0      1320.0      1320       1320          0.0  listen                \n",
      "      0.0              390          1       390.0       390.0       390        390          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     85.3        112691339          3  37563779.7    36901.0     17401  112637037   65015348.7  cudaMallocManaged    \n",
      "     11.9         15732069          3   5244023.0  4247589.0   4146667    7337813    1813977.3  cudaFree             \n",
      "      1.5          1981421          3    660473.7   645284.0    644153     691984      27294.6  cudaMemPrefetchAsync \n",
      "      1.3          1698716          2    849358.0   849358.0    837618     861098      16602.9  cudaDeviceSynchronize\n",
      "      0.0            44471          4     11117.8     6890.0      5400      25291       9481.1  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "     50.3           858083          1  858083.0  858083.0    858083    858083          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "     49.7           846562          3  282187.3  283328.0    279681    283553       2173.5  initWith(float, float *, int)                 \n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11075793    768   14421.6    3775.5      1439     80800      22785.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report16.nsys-rep\n",
      "    /dli/task/report16.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-de56.qdstrm'\n",
      "[1/8] [========================100%] report17.nsys-rep\n",
      "[2/8] [========================100%] report17.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report17.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     70.9        341809057         27  12659594.7  10071797.0      2300  100138589   19862244.7  poll                  \n",
      "     12.8         61944391         24   2581016.3   2064227.5       140   20554363    5267589.4  sem_timedwait         \n",
      "     12.1         58133158        501    116034.2     11360.0       370   22913821    1112628.3  ioctl                 \n",
      "      3.8         18378947         24    765789.5      4600.0       820    9925984    2275843.9  mmap                  \n",
      "      0.2           958373         27     35495.3      3771.0      3020     625973     118639.0  mmap64                \n",
      "      0.1           475118         44     10798.1     10375.0      3210      32820       4950.0  open64                \n",
      "      0.0           166295          4     41573.8     36196.0     31101      62802      14431.7  pthread_create        \n",
      "      0.0           156914         29      5410.8      3841.0      1440      28091       5572.2  fopen                 \n",
      "      0.0           148905         11     13536.8     13470.0      1040      22940       5360.2  write                 \n",
      "      0.0            51031         26      1962.7        75.0        60      49161       9626.6  fgets                 \n",
      "      0.0            38901          6      6483.5      7170.0      2920       8850       2497.5  open                  \n",
      "      0.0            35190         11      3199.1      3380.0      1250       4660        864.3  munmap                \n",
      "      0.0            33071         52       636.0       475.0       200       6121        829.3  fcntl                 \n",
      "      0.0            24921         22      1132.8       975.0       580       3080        585.6  fclose                \n",
      "      0.0            23790         14      1699.3      1335.0       830       3470        858.2  read                  \n",
      "      0.0            14740          2      7370.0      7370.0      4840       9900       3578.0  socket                \n",
      "      0.0             9660          5      1932.0      1570.0        90       3330       1341.4  fread                 \n",
      "      0.0             7980          1      7980.0      7980.0      7980       7980          0.0  pipe2                 \n",
      "      0.0             7821          1      7821.0      7821.0      7821       7821          0.0  connect               \n",
      "      0.0             7370         64       115.2       140.0        50        310         57.7  pthread_mutex_trylock \n",
      "      0.0             2400          1      2400.0      2400.0      2400       2400          0.0  bind                  \n",
      "      0.0             1340          1      1340.0      1340.0      1340       1340          0.0  listen                \n",
      "      0.0              250          1       250.0       250.0       250        250          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     71.2        110622174          3  36874058.0    26801.0     13690  110581683   63832676.0  cudaMallocManaged    \n",
      "     15.8         24609266          4   6152316.5   577621.5    507271   22946752   11196472.1  cudaMemPrefetchAsync \n",
      "     11.8         18404988          3   6134996.0  4253667.0   4187847    9963474    3315722.5  cudaFree             \n",
      "      1.1          1701375          2    850687.5   850687.5    837747     863628      18300.6  cudaDeviceSynchronize\n",
      "      0.0            39111          4      9777.8     5790.0      4310      23221       9005.4  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "     50.5           860291          1  860291.0  860291.0    860291    860291          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "     49.5           844067          3  281355.7  282529.0    278689    282849       2314.9  initWith(float, float *, int)                 \n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         10250714     64  160167.4  160160.0    160000    160577        123.3  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218     64     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report17.nsys-rep\n",
      "    /dli/task/report17.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](09-saxpy/01-saxpy.cu). It currently works and you can compile, run, and then profile it with `nsys profile` below.\n",
    "\n",
    "Record the runtime of the `saxpy` kernel without making any modifications and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200,000 ns*. Check out [the solution](09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Generating '/tmp/nsys-report-f53b.qdstrm'\n",
      "[1/8] [========================100%] report18.nsys-rep\n",
      "[2/8] [========================100%] report18.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report18.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     76.0        290758298         22  13216286.3  10069222.0      2130  100137888   21571960.4  poll                  \n",
      "     12.1         46416252         17   2730367.8   2054312.0       210   20435330    5202281.8  sem_timedwait         \n",
      "     10.3         39461509        497     79399.4     12880.0       380    9165978     517423.0  ioctl                 \n",
      "      0.9          3386710         23    147248.3      6230.0      1180    1183205     372979.6  mmap                  \n",
      "      0.3          1102552         27     40835.3      4370.0      3780     744765     141331.7  mmap64                \n",
      "      0.1           525924         44     11952.8     11086.0      4870      30971       4875.6  open64                \n",
      "      0.1           192524          4     48131.0     44141.0     38661      65581      12175.8  pthread_create        \n",
      "      0.0           171622         29      5918.0      4370.0      1510      32500       6022.9  fopen                 \n",
      "      0.0           157883         11     14353.0     16451.0       820      25080       6624.0  write                 \n",
      "      0.0            59122         26      2273.9        90.0        70      56852      11131.8  fgets                 \n",
      "      0.0            43950          6      7325.0      7975.0      3470       9600       2485.0  open                  \n",
      "      0.0            35620         52       685.0       525.0       210       5380        728.9  fcntl                 \n",
      "      0.0            34801          9      3866.8      3740.0      1850       5610       1272.2  munmap                \n",
      "      0.0            31710         22      1441.4      1265.0       710       3500        642.9  fclose                \n",
      "      0.0            26490         14      1892.1      1250.0       450       7060       1835.8  read                  \n",
      "      0.0            16330          2      8165.0      8165.0      4330      12000       5423.5  socket                \n",
      "      0.0            13461          1     13461.0     13461.0     13461      13461          0.0  connect               \n",
      "      0.0            10321          5      2064.2       230.0        80       6741       2918.6  fread                 \n",
      "      0.0             7440          1      7440.0      7440.0      7440       7440          0.0  pipe2                 \n",
      "      0.0             6650         64       103.9        60.0        50        490         69.0  pthread_mutex_trylock \n",
      "      0.0             2200          1      2200.0      2200.0      2200       2200          0.0  bind                  \n",
      "      0.0             1160          1      1160.0      1160.0      1160       1160          0.0  listen                \n",
      "      0.0              270          1       270.0       270.0       270        270          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)        Name       \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  -----------------\n",
      "     89.6        119407644          3  39802548.0    22460.0     18440  119366744   68904615.0  cudaMallocManaged\n",
      "     10.4         13804814          3   4601604.7  1079142.0   1067802   11657870    6110907.7  cudaFree         \n",
      "      0.0            50031          1     50031.0    50031.0     50031      50031          0.0  cudaLaunchKernel \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  --------------------------\n",
      "    100.0         10444827          1  10444827.0  10444827.0  10444827  10444827          0.0  saxpy(int *, int *, int *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0          7777827   2463    3157.9    2175.0      1823     68191       3298.0  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     50.332   2463     0.020     0.008     0.004     0.885        0.044  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report18.nsys-rep\n",
      "    /dli/task/report18.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
